{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook demonstrates the use of InFoRM algorithms to mitigate bias for spectral clustering\n",
    "InFoRM includes 3 algorithms, namely debiasing the input graph, debiasing the mining model and debiasing the mining result. We will show how to run all 3 algorithms for spectral clustering in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get 2-view data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get vanilla clustering membership matrix first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary packages\n",
    "import pickle5 as pickle\n",
    "import load_graph\n",
    "import utils\n",
    "import random\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse.csgraph import laplacian\n",
    "from scipy.sparse.linalg import eigsh\n",
    "import sklearn.preprocessing as skpp\n",
    "from sklearn.cluster import k_means\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from scipy import sparse\n",
    "from torch import optim\n",
    "import torch\n",
    "\n",
    "# load debias model\n",
    "from method.debias_model_fair import DebiasModel\n",
    "from data.synthetic.synthetic_data import *\n",
    "from evaluate.sc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 2\n",
    "adjs, sims, label  = synthetic_mat(1000)\n",
    "\n",
    "adj_1 = sparse.coo_matrix(adjs[0]).asfptype()\n",
    "adj_2 = sparse.coo_matrix(adjs[1]).asfptype()\n",
    "\n",
    "sim_1 = sparse.coo_matrix(sims[0]).asfptype()\n",
    "sim_2 = sparse.coo_matrix(sims[1]).asfptype()\n",
    "graph_syn = {'adjs': [adj_1, adj_2], \n",
    "             'sims': [sim_1, sim_2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla(graph, name):\n",
    "    udict = {}\n",
    "    if name == 'graph':\n",
    "        lcc = max(nx.connected_components(graph), key=len)  # take largest connected components\n",
    "        adj = nx.to_scipy_sparse_matrix(graph, nodelist=lcc, dtype='float', format='csc')\n",
    "    else:\n",
    "        adj_1 = graph['adjs'][0]\n",
    "        adj_2 = graph['adjs'][1]\n",
    "    ## unnormalized laplacian\n",
    "    lap_1 = laplacian(adj_1)\n",
    "    lap_2 = laplacian(adj_2)\n",
    "    \n",
    "    lap1_minus = (-1) * lap_1\n",
    "    lap2_minus = (-1) * lap_2\n",
    "    #lap *= -1\n",
    "    _, u_1 = eigsh(lap1_minus, which='LM', k=n_clusters, sigma=1.0)\n",
    "    _, u_2 = eigsh(lap2_minus, which='LM', k=n_clusters, sigma=1.0)\n",
    "    udict[name] = dict()\n",
    "    u = [u_1, u_2]\n",
    "    udict[name]['eigenvectors'] = u\n",
    "\n",
    "    with open('result/sc/vanilla_'+name+'.pickle', 'wb') as f:\n",
    "        pickle.dump(udict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_result = vanilla(syn, 'synthetic')\n",
    "vanilla_kmeans = [KMeans(n_clusters=vanilla_result[0].shape[1], random_state=0, n_init=200).fit(vanilla_result[0]),\n",
    "                 KMeans(n_clusters=vanilla_result[1].shape[1], random_state=0, n_init=200).fit(vanilla_result[1])]\n",
    "vanilla_labels = [vanilla_kmeans[0].labels_, vanilla_kmeans[1].labels_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2n+1 parameters\n",
    "def spectral_clustering(adjs, sims, alpha_l, alpha_s, alpha_u, lambda_s, lambda_u, ncluster, niteration, v0,\n",
    "                        max_patience, K_ini, K_min, mu,\n",
    "                        risk_round_factor, reset_optimizer):\n",
    "        ## initialize\n",
    "        i = 0\n",
    "        i_patience = 0\n",
    "\n",
    "        alpha_l = np.array(alpha_l)\n",
    "        alpha_s = np.array(alpha_s)\n",
    "        alpha_u = np.array(alpha_u)\n",
    "\n",
    "        L = {}\n",
    "        U = {}\n",
    "        \n",
    "        ## save \n",
    "        matrices = {0: {}}\n",
    "        matrices[0]['L'] = {}\n",
    "        matrices[0]['U'] = {}\n",
    "        matrices[0]['S'] = {}\n",
    "        matrices[0]['risk'] = []\n",
    "        matrices[0]['alpha_l'] = []\n",
    "        matrices[0]['alpha_s'] = []\n",
    "        matrices[0]['alpha_u'] = []\n",
    "        matrices[0]['risk_best'] = []\n",
    "        matrices[0]['alpha_s_best'] = []\n",
    "        matrices[0]['alpha_u_best'] = []\n",
    "        matrices[0]['params'] = []\n",
    "\n",
    "        L_mod = np.zeros((adjs[0].shape[0], adjs[0].shape[1]))\n",
    "        for v in range(len(adjs)):\n",
    "            # initial U\n",
    "            matrices[0]['S'][v] = alpha_s[v] * lambda_s * laplacian(sims[v]) \n",
    "            L[v] = alpha_l[v] * laplacian(adjs[v]) + alpha_s[v] * lambda_s * laplacian(sims[v])\n",
    "            L_mod += L[v]\n",
    "            L_minus = (-1) * L[v]\n",
    "\n",
    "            _, U[v] = eigsh(L_minus, which='LM', k=ncluster, sigma=1.0, v0=v0)\n",
    "        \n",
    "        L_mod_minus = (-1) * L_mod\n",
    "        _, U_mod = eigsh(L_mod_minus, which='LM', k=ncluster, sigma=1.0, v0=v0)\n",
    "        \n",
    "        K = K_ini\n",
    "        print(\"start iter\")\n",
    "        while( i <= niteration) & (i_patience <= max_patience):\n",
    "            matrices[i + 1] = {}\n",
    "            matrices[i + 1]['L'] = {}\n",
    "            matrices[i + 1]['U'] = {}\n",
    "            matrices[i + 1]['S'] = {}\n",
    "            matrices[i + 1]['KU'] = {}\n",
    "            matrices[i + 1]['params'] = []\n",
    "            matrices[i + 1]['risk'] = []\n",
    "            matrices[i + 1]['alpha_l'] = []\n",
    "            matrices[i + 1]['alpha_s'] = []\n",
    "            matrices[i + 1]['alpha_u'] = []\n",
    "            matrices[i + 1]['risk_best'] = []\n",
    "            matrices[i + 1]['alpha_l_best'] = []\n",
    "            matrices[i + 1]['alpha_s_best'] = []\n",
    "            matrices[i + 1]['alpha_u_best'] = []\n",
    "            #matrices[i + 1]['nmi_scores'] = {}\n",
    "            #matrices[i + 1]['labels'] = {}\n",
    "            \n",
    "            alpha_l_penalty = (alpha_l).astype('float16')\n",
    "            alpha_s_penalty = (alpha_s).astype('float16')\n",
    "            alpha_u_penalty = (alpha_u).astype('float16')\n",
    "            \n",
    "            print('#### Iteration:', i, '; current alpha: ', alpha_l_penalty, alpha_s_penalty, alpha_u_penalty)\n",
    "            loss = []\n",
    "            \n",
    "            \n",
    "            for v in range(len(adjs)):\n",
    "                u1 = sparse.csr_matrix(U[v])\n",
    "                loss.append((u1.T @ L[v] @ u1).diagonal().sum())\n",
    "            base_loss = np.array(loss)\n",
    "\n",
    "            # h,risk,_ = bs_optimal(mu_i)\n",
    "            risk = np.round(base_loss + 0, risk_round_factor)\n",
    "            risk_max = np.max(risk)\n",
    "\n",
    "            # argmax_risks\n",
    "            argrisk_max = np.arange(risk.shape[0])\n",
    "            argrisk_max = argrisk_max[risk == risk_max] #consider argmax exactly\n",
    "        \n",
    "            ## initialize\n",
    "            if i == 0:\n",
    "                risk_max_best = risk_max + 1\n",
    "                \n",
    "            if risk_max_best > risk_max: # risk is improved\n",
    "                # update best risk\n",
    "                risk_max_best = risk_max \n",
    "                argrisk_max_best = argrisk_max \n",
    "                risk_best = risk \n",
    "                alpha_l_best = alpha_l\n",
    "                alpha_s_best = alpha_s\n",
    "                alpha_u_best = alpha_u\n",
    "\n",
    "                ## resets\n",
    "                K = np.minimum(K, K_min)\n",
    "                i_patience = 0\n",
    "                type_step = 0 #improvement\n",
    "\n",
    "                print('Iteration:', i,' k:',K,'Improved minimax risk (arg/max): ', argrisk_max_best, risk_max_best)\n",
    "                #f.write('Iteration:', i,' k:',K,'Improved minimax risk (arg/max): ', argrisk_max_best, risk_max_best)\n",
    "            else:\n",
    "                K += 1\n",
    "                i_patience += 1\n",
    "                type_step = 1 # no improvement\n",
    "                print('Iteration: ', i,' k:',K, 'No minimax risk improvement, current best (arg/max): ', \n",
    "                      argrisk_max_best, risk_max_best)\n",
    "                #f.write('Iteration: ', i,' k:',K, 'No minimax risk improvement, current best (arg/max): ', \n",
    "                #      argrisk_max_best, risk_max_best)\n",
    "            \n",
    "            ## step update\n",
    "            mask_alpha_l = np.zeros(alpha_l.shape)\n",
    "            mask_alpha_s = np.zeros(alpha_s.shape)\n",
    "            mask_alpha_u = np.zeros(alpha_u.shape)\n",
    "            \n",
    "            mask_alpha_l[risk >= risk_max_best] = 1\n",
    "            mask_alpha_s[risk >= risk_max_best] = 1\n",
    "            mask_alpha_u[risk >= risk_max_best] = 1\n",
    "        \n",
    "            step_alpha_l = mask_alpha_l / np.sum(mask_alpha_l)\n",
    "            step_alpha_s = mask_alpha_s / np.sum(mask_alpha_s)\n",
    "            step_alpha_u = mask_alpha_u / np.sum(mask_alpha_u)\n",
    "            print('all_masks', mask_alpha_l, np.sum(mask_alpha_l), np.sum(mask_alpha_s), np.sum(mask_alpha_u))\n",
    "                \n",
    "            ##############    Save lists  ##############\n",
    "            #weight vector updated after save lists\n",
    "            matrices[i + 1]['params'].append([type_step, K, mu])\n",
    "            matrices[i + 1]['risk'].append(risk)\n",
    "            matrices[i + 1]['alpha_l'].append(alpha_l)\n",
    "            matrices[i + 1]['alpha_s'].append(alpha_s)\n",
    "            matrices[i + 1]['alpha_u'].append(alpha_u)\n",
    "            matrices[i + 1]['risk_best'].append(risk_best)\n",
    "            matrices[i + 1]['alpha_l_best'].append(alpha_l_best)\n",
    "            matrices[i + 1]['alpha_s_best'].append(alpha_s_best)\n",
    "            matrices[i + 1]['alpha_u_best'].append(alpha_u_best)\n",
    "            \n",
    "            print('Iteration:', i, '; step reduced minimax?:', type_step, ' risk (arg/ max)', argrisk_max,\n",
    "                  risk[argrisk_max], '; (arg/ max best): ', argrisk_max_best, risk_max_best)\n",
    "            print('alpha_l: ', alpha_l, 'alpha_s: ', alpha_s, '; new delta: ', step_alpha_l, step_alpha_s, ' mu: ', mu, '; K: ', K)\n",
    "            print('risks: ', risk, ' ; best risk: ', risk_best)\n",
    "            print()\n",
    "\n",
    "            \n",
    "            ############################################\n",
    "            \"\"\"for view, u in U.items():\n",
    "                view_kmeans = KMeans(n_clusters=u.shape[1], random_state=42, n_init=5).fit(u)\n",
    "                view_labels = view_kmeans.labels_\n",
    "                matrices[i + 1]['labels'][view] = view_labels\n",
    "                #matrices[i + 1]['nmi_scores'][view] = normalized_mutual_info_score(view_labels, vanilla_labels[view])\n",
    "                matrices[i + 1]['nmi_scores'][view] = normalized_mutual_info_score(label, view_labels)\n",
    "            \"\"\"\n",
    "            view_kmeans = KMeans(n_clusters=U_mod.shape[1], random_state=42, n_init=5).fit(U_mod)\n",
    "            view_labels = view_kmeans.labels_\n",
    "            matrices[i + 1]['labels'] = view_labels\n",
    "            matrices[i + 1]['nmi_scores'] = normalized_mutual_info_score(label, view_labels)\n",
    "            matrices[i + 1]['acc'] = acc(label, view_labels)\n",
    "            \n",
    "            print(\"NMI scores: \", matrices[i + 1]['nmi_scores'], \"acc: \", matrices[i + 1]['acc'])\n",
    "\n",
    "            ### UPDATE WEIGHTING_VECTOR ###\n",
    "            alpha_l = (1 - mu) * alpha_l + step_alpha_l * mu / K\n",
    "            alpha_s = (1 - mu) * alpha_s + step_alpha_s * mu / K\n",
    "            alpha_u = (1 - mu) * alpha_u + step_alpha_u * mu / K\n",
    "            \n",
    "\n",
    "            i += 1\n",
    "            L_mod = np.zeros((adjs[0].shape[0], adjs[0].shape[1]))\n",
    "            L_s = np.zeros((adjs[0].shape[0], adjs[0].shape[1]))\n",
    "            for j in range(len(adjs)):\n",
    "                KU = sum(np.matmul(U[k], U[k].T) for k in range(len(U)) if k != j)\n",
    "                lap = alpha_l[j] * L[j] - alpha_u[j] * lambda_u * KU + alpha_s[j] * lambda_s * laplacian(sims[j]) \n",
    "                L_mod += lap\n",
    "                lap_minus = (-1) * lap\n",
    "                _, U[j] = eigsh(lap_minus, which='LM', k=ncluster, sigma=1.0, v0=v0)\n",
    "                L[j] = np.array(lap)\n",
    "\n",
    "                matrices[i]['L'][j] = L[j]\n",
    "                matrices[i]['U'][j] = U[j]\n",
    "                matrices[i]['KU'][j] = KU\n",
    "                matrices[i]['S'][j] = alpha_s[j] * lambda_s * laplacian(sims[j]) \n",
    "                L_s += matrices[i]['S'][j]\n",
    "            L_mod_minus = (-1) * L_mod\n",
    "            _, U_mod = eigsh(L_mod_minus, which='LM', k=ncluster, sigma=1.0, v0=v0)\n",
    "            matrices[i]['L_mod'] = L_mod\n",
    "            matrices[i]['U_mod'] = U_mod\n",
    "            matrices[i]['L_s'] = L_s\n",
    "            print('Iteration: ', i+1)\n",
    "\n",
    "\n",
    "        #return matrices\n",
    "        return matrices\n",
    "    \n",
    "def debias_mining_model(adjs, sims, name, alpha, lmbda, niter=20, metric=None):\n",
    "    adjs = np.array(adjs)\n",
    "    sims = np.array(sims)\n",
    "    # debias spectral clustering\n",
    "    \n",
    "    alpha_l = [alpha, alpha]\n",
    "    alpha_s = [alpha, alpha]\n",
    "    alpha_u = [alpha, alpha]\n",
    "    lambda_s = lmbda\n",
    "    lambda_u = lmbda\n",
    "    \n",
    "    # V, U = sc.debias_alg(adj, sim, alpha, ncluster=10, v0=v0[name])\n",
    "    # model = DebiasModelFair(adjs, sims, ncluster=10, niteration=niter, v0=v0[name])\n",
    "    result = spectral_clustering(adjs, sims, alpha_l, alpha_s, alpha_u, lambda_s, lambda_u,\n",
    "                                 ncluster=n_clusters, niteration=niter, v0=None,\n",
    "                                 max_patience=15, K_ini=1, K_min=20, mu=0.5,\n",
    "                                 risk_round_factor=3, reset_optimizer=False)\n",
    "    \n",
    "\n",
    "    print('dataset: {}\\t metric: {} similarity'.format(name, metric))\n",
    "    print('Finished!')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluation\n",
    "def lp_diff(vanilla_result, fair_result, ord=None):\n",
    "    diffs = {}\n",
    "    if ord == 'fro':\n",
    "        diffs = 0\n",
    "        for i in range(vanilla_result.shape[1]):\n",
    "            residual = min(\n",
    "                np.linalg.norm(vanilla_result[:, i] + fair_result[:, i], ord=2),\n",
    "                np.linalg.norm(vanilla_result[:, i] - fair_result[:, i], ord=2)\n",
    "            )\n",
    "            diffs += (residual ** 2)\n",
    "        return np.sqrt(diffs)\n",
    "    else:\n",
    "        diff = vanilla_result - fair_result\n",
    "        return np.linalg.norm(diff, ord=ord)\n",
    "    \n",
    "def calc_bias(ls, vanilla_result, fair_result):\n",
    "    \n",
    "    # calculate bias\n",
    "    vanilla_bias = utils.trace(vanilla_result.T @ ls @ vanilla_result)  # vanilla bias\n",
    "    fair_bias = utils.trace(fair_result.T @ ls @ fair_result)  # fair bias\n",
    "    reduction = 1 - (fair_bias / vanilla_bias)\n",
    "    return reduction\n",
    "\n",
    "def acc(pred, true):\n",
    "    acc = 0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == true[i]:\n",
    "            acc += 1\n",
    "    return acc / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start iter\n",
      "#### Iteration: 0 ; current alpha:  [0.1 0.1] [0.1 0.1] [0.1 0.1]\n",
      "Iteration: 0  k: 1 Improved minimax risk (arg/max):  [1] 81.762\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 0 ; step reduced minimax?: 0  risk (arg/ max) [1] [81.762] ; (arg/ max best):  [1] 81.762\n",
      "alpha_l:  [0.1 0.1] alpha_s:  [0.1 0.1] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  1\n",
      "risks:  [32.392 81.762]  ; best risk:  [32.392 81.762]\n",
      "\n",
      "NMI scores:  0.001381429316719824 acc:  0.503\n",
      "Iteration:  2\n",
      "#### Iteration: 1 ; current alpha:  [0.05 0.55] [0.05 0.55] [0.05 0.55]\n",
      "Iteration:  1  k: 2 No minimax risk improvement, current best (arg/max):  [1] 81.762\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 1 ; step reduced minimax?: 1  risk (arg/ max) [1] [212.094] ; (arg/ max best):  [1] 81.762\n",
      "alpha_l:  [0.05 0.55] alpha_s:  [0.05 0.55] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  2\n",
      "risks:  [ 11.305 212.094]  ; best risk:  [32.392 81.762]\n",
      "\n",
      "NMI scores:  0.022137381541747343 acc:  0.537\n",
      "Iteration:  3\n",
      "#### Iteration: 2 ; current alpha:  [0.025 0.525] [0.025 0.525] [0.025 0.525]\n",
      "Iteration:  2  k: 3 No minimax risk improvement, current best (arg/max):  [1] 81.762\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 2 ; step reduced minimax?: 1  risk (arg/ max) [1] [265.157] ; (arg/ max best):  [1] 81.762\n",
      "alpha_l:  [0.025 0.525] alpha_s:  [0.025 0.525] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  3\n",
      "risks:  [  3.376 265.157]  ; best risk:  [32.392 81.762]\n",
      "\n",
      "NMI scores:  0.12484105004193616 acc:  0.69\n",
      "Iteration:  4\n",
      "#### Iteration: 3 ; current alpha:  [0.0125 0.4292] [0.0125 0.4292] [0.0125 0.4292]\n",
      "Iteration:  3  k: 4 No minimax risk improvement, current best (arg/max):  [1] 81.762\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 3 ; step reduced minimax?: 1  risk (arg/ max) [1] [236.348] ; (arg/ max best):  [1] 81.762\n",
      "alpha_l:  [0.0125     0.42916667] alpha_s:  [0.0125     0.42916667] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  4\n",
      "risks:  [  1.586 236.348]  ; best risk:  [32.392 81.762]\n",
      "\n",
      "NMI scores:  0.1627343112074982 acc:  0.727\n",
      "Iteration:  5\n",
      "#### Iteration: 4 ; current alpha:  [0.00625 0.3396 ] [0.00625 0.3396 ] [0.00625 0.3396 ]\n",
      "Iteration:  4  k: 5 No minimax risk improvement, current best (arg/max):  [1] 81.762\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 4 ; step reduced minimax?: 1  risk (arg/ max) [1] [177.068] ; (arg/ max best):  [1] 81.762\n",
      "alpha_l:  [0.00625    0.33958333] alpha_s:  [0.00625    0.33958333] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  5\n",
      "risks:  [  0.782 177.068]  ; best risk:  [32.392 81.762]\n",
      "\n",
      "NMI scores:  0.16993712403623346 acc:  0.734\n",
      "Iteration:  6\n",
      "#### Iteration: 5 ; current alpha:  [0.003124 0.2698  ] [0.003124 0.2698  ] [0.003124 0.2698  ]\n",
      "Iteration:  5  k: 6 No minimax risk improvement, current best (arg/max):  [1] 81.762\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 5 ; step reduced minimax?: 1  risk (arg/ max) [1] [124.666] ; (arg/ max best):  [1] 81.762\n",
      "alpha_l:  [0.003125   0.26979167] alpha_s:  [0.003125   0.26979167] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  6\n",
      "risks:  [  0.388 124.666]  ; best risk:  [32.392 81.762]\n",
      "\n",
      "NMI scores:  0.1773561032467258 acc:  0.74\n",
      "Iteration:  7\n",
      "#### Iteration: 6 ; current alpha:  [0.001562 0.2183  ] [0.001562 0.2183  ] [0.001562 0.2183  ]\n",
      "Iteration:  6  k: 7 No minimax risk improvement, current best (arg/max):  [1] 81.762\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 6 ; step reduced minimax?: 1  risk (arg/ max) [1] [89.402] ; (arg/ max best):  [1] 81.762\n",
      "alpha_l:  [0.0015625  0.21822917] alpha_s:  [0.0015625  0.21822917] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  7\n",
      "risks:  [ 0.194 89.402]  ; best risk:  [32.392 81.762]\n",
      "\n",
      "NMI scores:  0.17877173201066418 acc:  0.741\n",
      "Iteration:  8\n",
      "#### Iteration: 7 ; current alpha:  [0.000781 0.1805  ] [0.000781 0.1805  ] [0.000781 0.1805  ]\n",
      "Iteration: 7  k: 7 Improved minimax risk (arg/max):  [1] 67.596\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 7 ; step reduced minimax?: 0  risk (arg/ max) [1] [67.596] ; (arg/ max best):  [1] 67.596\n",
      "alpha_l:  [0.00078125 0.18054315] alpha_s:  [0.00078125 0.18054315] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  7\n",
      "risks:  [ 0.097 67.596]  ; best risk:  [ 0.097 67.596]\n",
      "\n",
      "NMI scores:  0.18019488838176945 acc:  0.742\n",
      "Iteration:  9\n",
      "#### Iteration: 8 ; current alpha:  [0.0003905 0.1617   ] [0.0003905 0.1617   ] [0.0003905 0.1617   ]\n",
      "Iteration: 8  k: 7 Improved minimax risk (arg/max):  [1] 57.016\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 8 ; step reduced minimax?: 0  risk (arg/ max) [1] [57.016] ; (arg/ max best):  [1] 57.016\n",
      "alpha_l:  [0.00039063 0.16170015] alpha_s:  [0.00039063 0.16170015] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  7\n",
      "risks:  [4.8000e-02 5.7016e+01]  ; best risk:  [4.8000e-02 5.7016e+01]\n",
      "\n",
      "NMI scores:  0.18019488838176945 acc:  0.742\n",
      "Iteration:  10\n",
      "#### Iteration: 9 ; current alpha:  [0.0001953 0.1522   ] [0.0001953 0.1522   ] [0.0001953 0.1522   ]\n",
      "Iteration: 9  k: 7 Improved minimax risk (arg/max):  [1] 52.082\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 9 ; step reduced minimax?: 0  risk (arg/ max) [1] [52.082] ; (arg/ max best):  [1] 52.082\n",
      "alpha_l:  [0.00019531 0.15227865] alpha_s:  [0.00019531 0.15227865] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  7\n",
      "risks:  [2.4000e-02 5.2082e+01]  ; best risk:  [2.4000e-02 5.2082e+01]\n",
      "\n",
      "NMI scores:  0.18019488838176945 acc:  0.742\n",
      "Iteration:  11\n",
      "#### Iteration: 10 ; current alpha:  [9.763e-05 1.476e-01] [9.763e-05 1.476e-01] [9.763e-05 1.476e-01]\n",
      "Iteration: 10  k: 7 Improved minimax risk (arg/max):  [1] 49.743\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 10 ; step reduced minimax?: 0  risk (arg/ max) [1] [49.743] ; (arg/ max best):  [1] 49.743\n",
      "alpha_l:  [9.76562500e-05 1.47567894e-01] alpha_s:  [9.76562500e-05 1.47567894e-01] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  7\n",
      "risks:  [1.2000e-02 4.9743e+01]  ; best risk:  [1.2000e-02 4.9743e+01]\n",
      "\n",
      "NMI scores:  0.18019488838176945 acc:  0.742\n",
      "Iteration:  12\n",
      "#### Iteration: 11 ; current alpha:  [4.882e-05 1.453e-01] [4.882e-05 1.453e-01] [4.882e-05 1.453e-01]\n",
      "Iteration: 11  k: 7 Improved minimax risk (arg/max):  [1] 48.609\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 11 ; step reduced minimax?: 0  risk (arg/ max) [1] [48.609] ; (arg/ max best):  [1] 48.609\n",
      "alpha_l:  [4.88281250e-05 1.45212519e-01] alpha_s:  [4.88281250e-05 1.45212519e-01] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  7\n",
      "risks:  [6.0000e-03 4.8609e+01]  ; best risk:  [6.0000e-03 4.8609e+01]\n",
      "\n",
      "NMI scores:  0.18019488838176945 acc:  0.742\n",
      "Iteration:  13\n",
      "#### Iteration: 12 ; current alpha:  [2.444e-05 1.440e-01] [2.444e-05 1.440e-01] [2.444e-05 1.440e-01]\n",
      "Iteration: 12  k: 7 Improved minimax risk (arg/max):  [1] 48.052\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 12 ; step reduced minimax?: 0  risk (arg/ max) [1] [48.052] ; (arg/ max best):  [1] 48.052\n",
      "alpha_l:  [2.44140625e-05 1.44034831e-01] alpha_s:  [2.44140625e-05 1.44034831e-01] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  7\n",
      "risks:  [3.0000e-03 4.8052e+01]  ; best risk:  [3.0000e-03 4.8052e+01]\n",
      "\n",
      "NMI scores:  0.18019488838176945 acc:  0.742\n",
      "Iteration:  14\n",
      "#### Iteration: 13 ; current alpha:  [1.222e-05 1.434e-01] [1.222e-05 1.434e-01] [1.222e-05 1.434e-01]\n",
      "Iteration: 13  k: 7 Improved minimax risk (arg/max):  [1] 47.776\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 13 ; step reduced minimax?: 0  risk (arg/ max) [1] [47.776] ; (arg/ max best):  [1] 47.776\n",
      "alpha_l:  [1.22070313e-05 1.43445987e-01] alpha_s:  [1.22070313e-05 1.43445987e-01] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  7\n",
      "risks:  [2.0000e-03 4.7776e+01]  ; best risk:  [2.0000e-03 4.7776e+01]\n",
      "\n",
      "NMI scores:  0.18019488838176945 acc:  0.742\n",
      "Iteration:  15\n",
      "#### Iteration: 14 ; current alpha:  [6.080e-06 1.432e-01] [6.080e-06 1.432e-01] [6.080e-06 1.432e-01]\n",
      "Iteration: 14  k: 7 Improved minimax risk (arg/max):  [1] 47.638\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 14 ; step reduced minimax?: 0  risk (arg/ max) [1] [47.638] ; (arg/ max best):  [1] 47.638\n",
      "alpha_l:  [6.10351563e-06 1.43151565e-01] alpha_s:  [6.10351563e-06 1.43151565e-01] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  7\n",
      "risks:  [1.0000e-03 4.7638e+01]  ; best risk:  [1.0000e-03 4.7638e+01]\n",
      "\n",
      "NMI scores:  0.18019488838176945 acc:  0.742\n",
      "Iteration:  16\n",
      "#### Iteration: 15 ; current alpha:  [3.04e-06 1.43e-01] [3.04e-06 1.43e-01] [3.04e-06 1.43e-01]\n",
      "Iteration: 15  k: 7 Improved minimax risk (arg/max):  [1] 47.569\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 15 ; step reduced minimax?: 0  risk (arg/ max) [1] [47.569] ; (arg/ max best):  [1] 47.569\n",
      "alpha_l:  [3.05175781e-06 1.43004354e-01] alpha_s:  [3.05175781e-06 1.43004354e-01] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  7\n",
      "risks:  [ 0.    47.569]  ; best risk:  [ 0.    47.569]\n",
      "\n",
      "NMI scores:  0.18019488838176945 acc:  0.742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  17\n",
      "#### Iteration: 16 ; current alpha:  [1.55e-06 1.43e-01] [1.55e-06 1.43e-01] [1.55e-06 1.43e-01]\n",
      "Iteration: 16  k: 7 Improved minimax risk (arg/max):  [1] 47.535\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 16 ; step reduced minimax?: 0  risk (arg/ max) [1] [47.535] ; (arg/ max best):  [1] 47.535\n",
      "alpha_l:  [1.52587891e-06 1.42930748e-01] alpha_s:  [1.52587891e-06 1.42930748e-01] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  7\n",
      "risks:  [ 0.    47.535]  ; best risk:  [ 0.    47.535]\n",
      "\n",
      "NMI scores:  0.18019488838176945 acc:  0.742\n",
      "Iteration:  18\n",
      "#### Iteration: 17 ; current alpha:  [7.75e-07 1.43e-01] [7.75e-07 1.43e-01] [7.75e-07 1.43e-01]\n",
      "Iteration: 17  k: 7 Improved minimax risk (arg/max):  [1] 47.518\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 17 ; step reduced minimax?: 0  risk (arg/ max) [1] [47.518] ; (arg/ max best):  [1] 47.518\n",
      "alpha_l:  [7.62939453e-07 1.42893946e-01] alpha_s:  [7.62939453e-07 1.42893946e-01] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  7\n",
      "risks:  [ 0.    47.518]  ; best risk:  [ 0.    47.518]\n",
      "\n",
      "NMI scores:  0.18019488838176945 acc:  0.742\n",
      "Iteration:  19\n",
      "#### Iteration: 18 ; current alpha:  [3.576e-07 1.428e-01] [3.576e-07 1.428e-01] [3.576e-07 1.428e-01]\n",
      "Iteration: 18  k: 7 Improved minimax risk (arg/max):  [1] 47.509\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 18 ; step reduced minimax?: 0  risk (arg/ max) [1] [47.509] ; (arg/ max best):  [1] 47.509\n",
      "alpha_l:  [3.81469727e-07 1.42875544e-01] alpha_s:  [3.81469727e-07 1.42875544e-01] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  7\n",
      "risks:  [ 0.    47.509]  ; best risk:  [ 0.    47.509]\n",
      "\n",
      "NMI scores:  0.18019488838176945 acc:  0.258\n",
      "Iteration:  20\n",
      "#### Iteration: 19 ; current alpha:  [1.788e-07 1.428e-01] [1.788e-07 1.428e-01] [1.788e-07 1.428e-01]\n",
      "Iteration: 19  k: 7 Improved minimax risk (arg/max):  [1] 47.505\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 19 ; step reduced minimax?: 0  risk (arg/ max) [1] [47.505] ; (arg/ max best):  [1] 47.505\n",
      "alpha_l:  [1.90734863e-07 1.42866344e-01] alpha_s:  [1.90734863e-07 1.42866344e-01] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  7\n",
      "risks:  [ 0.    47.505]  ; best risk:  [ 0.    47.505]\n",
      "\n",
      "NMI scores:  0.18019488838176945 acc:  0.742\n",
      "Iteration:  21\n",
      "#### Iteration: 20 ; current alpha:  [1.192e-07 1.428e-01] [1.192e-07 1.428e-01] [1.192e-07 1.428e-01]\n",
      "Iteration: 20  k: 7 Improved minimax risk (arg/max):  [1] 47.503\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 20 ; step reduced minimax?: 0  risk (arg/ max) [1] [47.503] ; (arg/ max best):  [1] 47.503\n",
      "alpha_l:  [9.53674316e-08 1.42861743e-01] alpha_s:  [9.53674316e-08 1.42861743e-01] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  7\n",
      "risks:  [ 0.    47.503]  ; best risk:  [ 0.    47.503]\n",
      "\n",
      "NMI scores:  0.18019488838176945 acc:  0.742\n",
      "Iteration:  22\n",
      "dataset: syn\t metric: jaccard similarity\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "lmbda = 0.5\n",
    "result = dict()\n",
    "\n",
    "result['syn'] = debias_mining_model(adjs, sims, name='syn', alpha=alpha, lmbda=lmbda, metric='jaccard')\n",
    "with open('result/sc/model/jaccard_syn.pickle', 'wb') as f:\n",
    "    pickle.dump(result, f, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate for synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduce_0: 0.06337793597021757\n",
      "reduce_1: 0.06838971008411909\n",
      "0.742 0.499\n",
      "0.7411291584891575 0.33288859239492996\n",
      "0.18019488838176945 0.001978869673592188\n",
      "0.23349905614899846 0.0\n"
     ]
    }
   ],
   "source": [
    "#vanilla_result = result['vanilla_ppi']\n",
    "l = result['syn'][21]['L_mod']\n",
    "ls = result['syn'][21]['L_s']\n",
    "u = result['syn'][21]['U_mod']\n",
    "fair_labels = result['syn'][21]['labels']\n",
    "\n",
    "#vanilla_kmeans = KMeans(n_clusters=u.shape[1], random_state=42, n_init=5).fit(vanilla_result[0])\n",
    "#vanilla_labels = vanilla_kmeans.labels_\n",
    "\n",
    "# reduce\n",
    "for k in range(2):\n",
    "    bias = calc_bias(ls, vanilla_result[k], u)\n",
    "    print('reduce_{}:'.format(k), bias)\n",
    "    \n",
    "# accuracy\n",
    "print(acc(fair_labels, label), acc(vanilla_labels[0], label))\n",
    "\n",
    "# f1-score\n",
    "print(f1_score(label, fair_labels, average='macro'), f1_score(label, vanilla_labels[0], average='macro'))\n",
    "\n",
    "# nmi\n",
    "print(normalized_mutual_info_score(label, fair_labels), normalized_mutual_info_score(label, vanilla_labels[0]))\n",
    "\n",
    "# ari\n",
    "print(adjusted_rand_score(label, fair_labels), adjusted_rand_score(label, vanilla_labels[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla(graph, name, n_clusters):\n",
    "    udict = {}\n",
    "    if name == 'graph':\n",
    "        lcc = max(nx.connected_components(graph), key=len)  # take largest connected components\n",
    "        adj = nx.to_scipy_sparse_matrix(graph, nodelist=lcc, dtype='float', format='csc')\n",
    "    elif name == 'synthetic':\n",
    "        adj_1 = graph['adjs'][0]\n",
    "        adj_2 = graph['adjs'][1]\n",
    "        adjs = [adj_1, adj_2]\n",
    "    elif name == 'imdb':\n",
    "        #lcc_1 = max(nx.connected_components(graph[0]), key=len)  # take largest connected components\n",
    "        #lcc_2 = max(nx.connected_components(graph[1]), key=len)\n",
    "        adj_1 = nx.to_scipy_sparse_matrix(graph[0], nodelist=set(graph[0].nodes), dtype='float', format='csc')\n",
    "        adj_2 = nx.to_scipy_sparse_matrix(graph[1], nodelist=set(graph[1].nodes), dtype='float', format='csc')\n",
    "        adjs = [adj_1, adj_2]\n",
    "    elif name == 'dblp':\n",
    "        #lcc_1 = max(nx.connected_components(graph[0]), key=len)  # take largest connected components\n",
    "        #lcc_2 = max(nx.connected_components(graph[1]), key=len)\n",
    "        adj_1 = nx.to_scipy_sparse_matrix(graph[0], nodelist=set(graph[0].nodes), dtype='float', format='csc')\n",
    "        adj_2 = nx.to_scipy_sparse_matrix(graph[1], nodelist=set(graph[1].nodes), dtype='float', format='csc')\n",
    "        adj_3 = nx.to_scipy_sparse_matrix(graph[2], nodelist=set(graph[2].nodes), dtype='float', format='csc')\n",
    "        adjs = [adj_1, adj_2, adj_3]\n",
    "    ## unnormalized laplacian\n",
    "    laps = [laplacian(adj) for adj in adjs]\n",
    "    \n",
    "    laps_minus = [(-1) * lap for lap in laps]\n",
    "    #lap *= -1\n",
    "    results = [eigsh(lap_minus, which='LM', k=n_clusters, sigma=1.0) for lap_minus in laps_minus]\n",
    "    u = [results[i][1] for i in range(len(results))]\n",
    "    udict[name] = dict()\n",
    "    udict[name]['eigenvectors'] = u\n",
    "\n",
    "    with open('result/sc/vanilla_'+name+'.pickle', 'wb') as f:\n",
    "        pickle.dump(udict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return u\n",
    "\n",
    "## 2n+1 parameters\n",
    "def spectral_clustering(adjs, sims, alpha_l, alpha_s, alpha_u, lambda_s, lambda_u, ncluster, niteration, v0,\n",
    "                        max_patience, K_ini, K_min, mu,\n",
    "                        risk_round_factor, reset_optimizer):\n",
    "        ## initialize\n",
    "        i = 0\n",
    "        i_patience = 0\n",
    "\n",
    "        alpha_l = np.array(alpha_l)\n",
    "        alpha_s = np.array(alpha_s)\n",
    "        alpha_u = np.array(alpha_u)\n",
    "\n",
    "        L = {}\n",
    "        U = {}\n",
    "        \n",
    "        ## save \n",
    "        matrices = {0: {}}\n",
    "        matrices[0]['L'] = {}\n",
    "        matrices[0]['U'] = {}\n",
    "        matrices[0]['S'] = {}\n",
    "        matrices[0]['risk'] = []\n",
    "        matrices[0]['alpha_l'] = []\n",
    "        matrices[0]['alpha_s'] = []\n",
    "        matrices[0]['alpha_u'] = []\n",
    "        matrices[0]['risk_best'] = []\n",
    "        matrices[0]['alpha_s_best'] = []\n",
    "        matrices[0]['alpha_u_best'] = []\n",
    "        matrices[0]['params'] = []\n",
    "\n",
    "        L_mod = np.zeros((adjs[0].shape[0], adjs[0].shape[1]))\n",
    "        for v in range(len(adjs)):\n",
    "            # initial U\n",
    "            matrices[0]['S'][v] = alpha_s[v] * lambda_s * laplacian(sims[v]) \n",
    "            L[v] = alpha_l[v] * laplacian(adjs[v]) + alpha_s[v] * lambda_s * laplacian(sims[v])\n",
    "            L_mod += L[v]\n",
    "            L_minus = (-1) * L[v]\n",
    "\n",
    "            _, U[v] = eigsh(L_minus, which='LM', k=ncluster, sigma=1.0, v0=v0)\n",
    "        \n",
    "        L_mod_minus = (-1) * L_mod\n",
    "        _, U_mod = eigsh(L_mod_minus, which='LM', k=ncluster, sigma=1.0, v0=v0)\n",
    "        \n",
    "        K = K_ini\n",
    "        print(\"start iter\")\n",
    "        while( i <= niteration) & (i_patience <= max_patience):\n",
    "            matrices[i + 1] = {}\n",
    "            matrices[i + 1]['L'] = {}\n",
    "            matrices[i + 1]['U'] = {}\n",
    "            matrices[i + 1]['S'] = {}\n",
    "            matrices[i + 1]['KU'] = {}\n",
    "            matrices[i + 1]['params'] = []\n",
    "            matrices[i + 1]['risk'] = []\n",
    "            matrices[i + 1]['alpha_l'] = []\n",
    "            matrices[i + 1]['alpha_s'] = []\n",
    "            matrices[i + 1]['alpha_u'] = []\n",
    "            matrices[i + 1]['risk_best'] = []\n",
    "            matrices[i + 1]['alpha_l_best'] = []\n",
    "            matrices[i + 1]['alpha_s_best'] = []\n",
    "            matrices[i + 1]['alpha_u_best'] = []\n",
    "            #matrices[i + 1]['nmi_scores'] = {}\n",
    "            #matrices[i + 1]['labels'] = {}\n",
    "            \n",
    "            alpha_l_penalty = (alpha_l).astype('float16')\n",
    "            alpha_s_penalty = (alpha_s).astype('float16')\n",
    "            alpha_u_penalty = (alpha_u).astype('float16')\n",
    "            \n",
    "            print('#### Iteration:', i, '; current alpha: ', alpha_l_penalty, alpha_s_penalty, alpha_u_penalty)\n",
    "            loss = []\n",
    "            \n",
    "            \n",
    "            for v in range(len(adjs)):\n",
    "                u1 = sparse.csr_matrix(U[v])\n",
    "                loss.append((u1.T @ L[v] @ u1).diagonal().sum())\n",
    "            base_loss = np.array(loss)\n",
    "\n",
    "            # h,risk,_ = bs_optimal(mu_i)\n",
    "            risk = np.round(base_loss + 0, risk_round_factor)\n",
    "            risk_max = np.max(risk)\n",
    "\n",
    "            # argmax_risks\n",
    "            argrisk_max = np.arange(risk.shape[0])\n",
    "            argrisk_max = argrisk_max[risk == risk_max] #consider argmax exactly\n",
    "        \n",
    "            ## initialize\n",
    "            if i == 0:\n",
    "                risk_max_best = risk_max + 1\n",
    "                \n",
    "            if risk_max_best > risk_max: # risk is improved\n",
    "                # update best risk\n",
    "                risk_max_best = risk_max \n",
    "                argrisk_max_best = argrisk_max \n",
    "                risk_best = risk \n",
    "                alpha_l_best = alpha_l\n",
    "                alpha_s_best = alpha_s\n",
    "                alpha_u_best = alpha_u\n",
    "\n",
    "                ## resets\n",
    "                K = np.minimum(K, K_min)\n",
    "                i_patience = 0\n",
    "                type_step = 0 #improvement\n",
    "\n",
    "                print('Iteration:', i,' k:',K,'Improved minimax risk (arg/max): ', argrisk_max_best, risk_max_best)\n",
    "                #f.write('Iteration:', i,' k:',K,'Improved minimax risk (arg/max): ', argrisk_max_best, risk_max_best)\n",
    "            else:\n",
    "                K += 1\n",
    "                i_patience += 1\n",
    "                type_step = 1 # no improvement\n",
    "                print('Iteration: ', i,' k:',K, 'No minimax risk improvement, current best (arg/max): ', \n",
    "                      argrisk_max_best, risk_max_best)\n",
    "                #f.write('Iteration: ', i,' k:',K, 'No minimax risk improvement, current best (arg/max): ', \n",
    "                #      argrisk_max_best, risk_max_best)\n",
    "            \n",
    "            ## step update\n",
    "            mask_alpha_l = np.zeros(alpha_l.shape)\n",
    "            mask_alpha_s = np.zeros(alpha_s.shape)\n",
    "            mask_alpha_u = np.zeros(alpha_u.shape)\n",
    "            \n",
    "            mask_alpha_l[risk >= risk_max_best] = 1\n",
    "            mask_alpha_s[risk >= risk_max_best] = 1\n",
    "            mask_alpha_u[risk >= risk_max_best] = 1\n",
    "        \n",
    "            step_alpha_l = mask_alpha_l / np.sum(mask_alpha_l)\n",
    "            step_alpha_s = mask_alpha_s / np.sum(mask_alpha_s)\n",
    "            step_alpha_u = mask_alpha_u / np.sum(mask_alpha_u)\n",
    "            print('all_masks', mask_alpha_l, np.sum(mask_alpha_l), np.sum(mask_alpha_s), np.sum(mask_alpha_u))\n",
    "                \n",
    "            ##############    Save lists  ##############\n",
    "            #weight vector updated after save lists\n",
    "            matrices[i + 1]['params'].append([type_step, K, mu])\n",
    "            matrices[i + 1]['risk'].append(risk)\n",
    "            matrices[i + 1]['alpha_l'].append(alpha_l)\n",
    "            matrices[i + 1]['alpha_s'].append(alpha_s)\n",
    "            matrices[i + 1]['alpha_u'].append(alpha_u)\n",
    "            matrices[i + 1]['risk_best'].append(risk_best)\n",
    "            matrices[i + 1]['alpha_l_best'].append(alpha_l_best)\n",
    "            matrices[i + 1]['alpha_s_best'].append(alpha_s_best)\n",
    "            matrices[i + 1]['alpha_u_best'].append(alpha_u_best)\n",
    "            \n",
    "            print('Iteration:', i, '; step reduced minimax?:', type_step, ' risk (arg/ max)', argrisk_max,\n",
    "                  risk[argrisk_max], '; (arg/ max best): ', argrisk_max_best, risk_max_best)\n",
    "            print('alpha_l: ', alpha_l, 'alpha_s: ', alpha_s, '; new delta: ', step_alpha_l, step_alpha_s, ' mu: ', mu, '; K: ', K)\n",
    "            print('risks: ', risk, ' ; best risk: ', risk_best)\n",
    "            print()\n",
    "\n",
    "            \n",
    "            ############################################\n",
    "            \"\"\"for view, u in U.items():\n",
    "                view_kmeans = KMeans(n_clusters=u.shape[1], random_state=42, n_init=5).fit(u)\n",
    "                view_labels = view_kmeans.labels_\n",
    "                matrices[i + 1]['labels'][view] = view_labels\n",
    "                #matrices[i + 1]['nmi_scores'][view] = normalized_mutual_info_score(view_labels, vanilla_labels[view])\n",
    "                matrices[i + 1]['nmi_scores'][view] = normalized_mutual_info_score(label, view_labels)\n",
    "            \"\"\"\n",
    "            view_kmeans = KMeans(n_clusters=U_mod.shape[1], random_state=42, n_init=5).fit(U_mod)\n",
    "            view_labels = view_kmeans.labels_\n",
    "            matrices[i + 1]['labels'] = view_labels\n",
    "            matrices[i + 1]['nmi_scores'] = normalized_mutual_info_score(label, view_labels)\n",
    "            matrices[i + 1]['acc'] = acc(label, view_labels)\n",
    "            \n",
    "            print(\"NMI scores: \", matrices[i + 1]['nmi_scores'], \"acc: \", matrices[i + 1]['acc'])\n",
    "\n",
    "            ### UPDATE WEIGHTING_VECTOR ###\n",
    "            alpha_l = (1 - mu) * alpha_l + step_alpha_l * mu / K\n",
    "            alpha_s = (1 - mu) * alpha_s + step_alpha_s * mu / K\n",
    "            alpha_u = (1 - mu) * alpha_u + step_alpha_u * mu / K\n",
    "            \n",
    "\n",
    "            i += 1\n",
    "            L_mod = np.zeros((adjs[0].shape[0], adjs[0].shape[1]))\n",
    "            L_s = np.zeros((adjs[0].shape[0], adjs[0].shape[1]))\n",
    "            for j in range(len(adjs)):\n",
    "                KU = sum(np.matmul(U[k], U[k].T) for k in range(len(U)) if k != j)\n",
    "                lap = alpha_l[j] * L[j] - alpha_u[j] * lambda_u * KU + alpha_s[j] * lambda_s * laplacian(sims[j]) \n",
    "                L_mod += lap\n",
    "                lap_minus = (-1) * lap\n",
    "                _, U[j] = eigsh(lap_minus, which='LM', k=ncluster, sigma=1.0, v0=v0)\n",
    "                L[j] = np.array(lap)\n",
    "\n",
    "                matrices[i]['L'][j] = L[j]\n",
    "                matrices[i]['U'][j] = U[j]\n",
    "                matrices[i]['KU'][j] = KU\n",
    "                matrices[i]['S'][j] = alpha_s[j] * lambda_s * laplacian(sims[j]) \n",
    "                L_s += matrices[i]['S'][j]\n",
    "            L_mod_minus = (-1) * L_mod\n",
    "            _, U_mod = eigsh(L_mod_minus, which='LM', k=ncluster, sigma=1.0, v0=v0)\n",
    "            matrices[i]['L_mod'] = L_mod\n",
    "            matrices[i]['U_mod'] = U_mod\n",
    "            matrices[i]['L_s'] = L_s\n",
    "            print('Iteration: ', i+1)\n",
    "\n",
    "\n",
    "\n",
    "        #return matrices\n",
    "        return matrices\n",
    "    \n",
    "def debias_mining_model(graphs, name, alpha_l, alpha_s, alpha_u, lambda_s, lambda_u, ncluster, niter=10, metric=None):\n",
    "    udict = {}\n",
    "\n",
    "    if name == 'synthetic':\n",
    "        adj_1 = graphs['adjs'][0]\n",
    "        adj_2 = graphs['adjs'][1]\n",
    "        \n",
    "        sim_1 = graphs['sims'][0]\n",
    "        sim_2 = graphs['sims'][0]\n",
    "        \n",
    "        adjs = np.array([adj_1, adj_2])\n",
    "        sims = np.array([sim_1, sim_2])\n",
    "    elif name == 'imdb':\n",
    "        # build adjacency matrix\n",
    "        adj_1 = nx.to_scipy_sparse_matrix(graphs[0], nodelist=set(graphs[0].nodes), dtype='float', format='csc')\n",
    "        adj_2 = nx.to_scipy_sparse_matrix(graphs[1], nodelist=set(graphs[1].nodes), dtype='float', format='csc')\n",
    "\n",
    "        # build similarity matrix\n",
    "        sim_1 = utils.get_similarity_matrix(adj_1, metric=metric)\n",
    "        sim_2 = utils.get_similarity_matrix(adj_2, metric=metric)\n",
    "    \n",
    "        adjs = np.array([adj_1, adj_2])\n",
    "        sims = np.array([sim_1, sim_2])\n",
    "    \n",
    "    # debias spectral clustering\n",
    "    \n",
    "    #alpha_l = alpha_\n",
    "    #alpha_s = [alpha, alpha]\n",
    "    #alpha_u = [alpha, alpha]\n",
    "    #lambda_s = lmbda\n",
    "    #lambda_u = lmbda\n",
    "    \n",
    "    # V, U = sc.debias_alg(adj, sim, alpha, ncluster=10, v0=v0[name])\n",
    "    # model = DebiasModelFair(adjs, sims, ncluster=10, niteration=niter, v0=v0[name])\n",
    "    result = spectral_clustering(adjs, sims, alpha_l, alpha_s, alpha_u, lambda_s, lambda_u,\n",
    "                                 ncluster=n_clusters, niteration=niter, v0=None,\n",
    "                                 max_patience=15, K_ini=1, K_min=20, mu=0.5,\n",
    "                                 risk_round_factor=3, reset_optimizer=False)\n",
    "    \n",
    "\n",
    "    print('dataset: {}\\t metric: {} similarity'.format(name, metric))\n",
    "    print('Finished!')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/IMDB/imdb.pkl'\n",
    "imdb = pickle.load(open('data/IMDB/imdb.pkl', 'rb'))\n",
    "G_mdm = nx.convert_matrix.from_numpy_array(imdb['MDM'].astype(int))\n",
    "G_mam = nx.convert_matrix.from_numpy_array(imdb['MAM'].astype(int))\n",
    "n_clusters = 3\n",
    "\n",
    "vanilla_result = vanilla([G_mdm, G_mam], 'imdb', 3)\n",
    "vanilla_kmeans = [KMeans(n_clusters=n_clusters, random_state=0, n_init=200).fit(vanilla_result[0]),\n",
    "                 KMeans(n_clusters=n_clusters, random_state=0, n_init=200).fit(vanilla_result[1])]\n",
    "vanilla_labels = [vanilla_kmeans[0].labels_, vanilla_kmeans[1].labels_]\n",
    "\n",
    "## vanilla\n",
    "label = []\n",
    "for li in imdb['label'].astype(int):\n",
    "    if li[0] == 1:\n",
    "        label.append(0)\n",
    "    elif li[1] == 1:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start iter\n",
      "#### Iteration: 0 ; current alpha:  [0.1 0.1] [0.1 0.1] [0.1 0.1]\n",
      "Iteration: 0  k: 1 Improved minimax risk (arg/max):  [0 1] -0.0\n",
      "all_masks [1. 1.] 2.0 2.0 2.0\n",
      "Iteration: 0 ; step reduced minimax?: 0  risk (arg/ max) [0 1] [-0.  0.] ; (arg/ max best):  [0 1] -0.0\n",
      "alpha_l:  [0.1 0.1] alpha_s:  [0.1 0.1] ; new delta:  [0.5 0.5] [0.5 0.5]  mu:  0.5 ; K:  1\n",
      "risks:  [-0.  0.]  ; best risk:  [-0.  0.]\n",
      "\n",
      "NMI scores:  0.0015160559246752108 acc:  0.38\n",
      "Iteration:  2\n",
      "#### Iteration: 1 ; current alpha:  [0.3 0.3] [0.3 0.3] [0.3 0.3]\n",
      "Iteration:  1  k: 2 No minimax risk improvement, current best (arg/max):  [0 1] -0.0\n",
      "all_masks [1. 1.] 2.0 2.0 2.0\n",
      "Iteration: 1 ; step reduced minimax?: 1  risk (arg/ max) [0 1] [-0.  0.] ; (arg/ max best):  [0 1] -0.0\n",
      "alpha_l:  [0.3 0.3] alpha_s:  [0.3 0.3] ; new delta:  [0.5 0.5] [0.5 0.5]  mu:  0.5 ; K:  2\n",
      "risks:  [-0.  0.]  ; best risk:  [-0.  0.]\n",
      "\n",
      "NMI scores:  0.0008286989659244619 acc:  0.37690140845070425\n",
      "Iteration:  3\n",
      "#### Iteration: 2 ; current alpha:  [0.275 0.275] [0.275 0.275] [0.275 0.275]\n",
      "Iteration:  2  k: 3 No minimax risk improvement, current best (arg/max):  [0 1] -0.0\n",
      "all_masks [1. 1.] 2.0 2.0 2.0\n",
      "Iteration: 2 ; step reduced minimax?: 1  risk (arg/ max) [0 1] [ 0. -0.] ; (arg/ max best):  [0 1] -0.0\n",
      "alpha_l:  [0.275 0.275] alpha_s:  [0.275 0.275] ; new delta:  [0.5 0.5] [0.5 0.5]  mu:  0.5 ; K:  3\n",
      "risks:  [ 0. -0.]  ; best risk:  [-0.  0.]\n",
      "\n",
      "NMI scores:  0.0004258653918936332 acc:  0.37690140845070425\n",
      "Iteration:  4\n",
      "#### Iteration: 3 ; current alpha:  [0.2208 0.2208] [0.2208 0.2208] [0.2208 0.2208]\n",
      "Iteration:  3  k: 4 No minimax risk improvement, current best (arg/max):  [0 1] -0.0\n",
      "all_masks [1. 1.] 2.0 2.0 2.0\n",
      "Iteration: 3 ; step reduced minimax?: 1  risk (arg/ max) [0 1] [ 0. -0.] ; (arg/ max best):  [0 1] -0.0\n",
      "alpha_l:  [0.22083333 0.22083333] alpha_s:  [0.22083333 0.22083333] ; new delta:  [0.5 0.5] [0.5 0.5]  mu:  0.5 ; K:  4\n",
      "risks:  [ 0. -0.]  ; best risk:  [-0.  0.]\n",
      "\n",
      "NMI scores:  0.0009635538345172175 acc:  0.3749295774647887\n",
      "Iteration:  5\n",
      "#### Iteration: 4 ; current alpha:  [0.173 0.173] [0.173 0.173] [0.173 0.173]\n",
      "Iteration:  4  k: 5 No minimax risk improvement, current best (arg/max):  [0 1] -0.0\n",
      "all_masks [1. 1.] 2.0 2.0 2.0\n",
      "Iteration: 4 ; step reduced minimax?: 1  risk (arg/ max) [0 1] [-0. -0.] ; (arg/ max best):  [0 1] -0.0\n",
      "alpha_l:  [0.17291667 0.17291667] alpha_s:  [0.17291667 0.17291667] ; new delta:  [0.5 0.5] [0.5 0.5]  mu:  0.5 ; K:  5\n",
      "risks:  [-0. -0.]  ; best risk:  [-0.  0.]\n",
      "\n",
      "NMI scores:  0.002312785681173036 acc:  0.3780281690140845\n",
      "Iteration:  6\n",
      "#### Iteration: 5 ; current alpha:  [0.1365 0.1365] [0.1365 0.1365] [0.1365 0.1365]\n",
      "Iteration:  5  k: 6 No minimax risk improvement, current best (arg/max):  [0 1] -0.0\n",
      "all_masks [1. 1.] 2.0 2.0 2.0\n",
      "Iteration: 5 ; step reduced minimax?: 1  risk (arg/ max) [0 1] [-0. -0.] ; (arg/ max best):  [0 1] -0.0\n",
      "alpha_l:  [0.13645833 0.13645833] alpha_s:  [0.13645833 0.13645833] ; new delta:  [0.5 0.5] [0.5 0.5]  mu:  0.5 ; K:  6\n",
      "risks:  [-0. -0.]  ; best risk:  [-0.  0.]\n",
      "\n",
      "NMI scores:  0.0015611288455454812 acc:  0.37971830985915495\n",
      "Iteration:  7\n",
      "#### Iteration: 6 ; current alpha:  [0.1099 0.1099] [0.1099 0.1099] [0.1099 0.1099]\n",
      "Iteration:  6  k: 7 No minimax risk improvement, current best (arg/max):  [0 1] -0.0\n",
      "all_masks [1. 1.] 2.0 2.0 2.0\n",
      "Iteration: 6 ; step reduced minimax?: 1  risk (arg/ max) [0 1] [-0. -0.] ; (arg/ max best):  [0 1] -0.0\n",
      "alpha_l:  [0.10989583 0.10989583] alpha_s:  [0.10989583 0.10989583] ; new delta:  [0.5 0.5] [0.5 0.5]  mu:  0.5 ; K:  7\n",
      "risks:  [-0. -0.]  ; best risk:  [-0.  0.]\n",
      "\n",
      "NMI scores:  0.0019764411503921183 acc:  0.37690140845070425\n",
      "Iteration:  8\n",
      "#### Iteration: 7 ; current alpha:  [0.09064 0.09064] [0.09064 0.09064] [0.09064 0.09064]\n",
      "Iteration:  7  k: 8 No minimax risk improvement, current best (arg/max):  [0 1] -0.0\n",
      "all_masks [1. 0.] 1.0 1.0 1.0\n",
      "Iteration: 7 ; step reduced minimax?: 1  risk (arg/ max) [0] [0.] ; (arg/ max best):  [0 1] -0.0\n",
      "alpha_l:  [0.0906622 0.0906622] alpha_s:  [0.0906622 0.0906622] ; new delta:  [1. 0.] [1. 0.]  mu:  0.5 ; K:  8\n",
      "risks:  [ 0.    -0.449]  ; best risk:  [-0.  0.]\n",
      "\n",
      "NMI scores:  0.0007062861369660176 acc:  0.3771830985915493\n",
      "Iteration:  9\n",
      "#### Iteration: 8 ; current alpha:  [0.10785 0.04532] [0.10785 0.04532] [0.10785 0.04532]\n",
      "Iteration: 8  k: 8 Improved minimax risk (arg/max):  [1] -1.405\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 8 ; step reduced minimax?: 0  risk (arg/ max) [1] [-1.405] ; (arg/ max best):  [1] -1.405\n",
      "alpha_l:  [0.1078311 0.0453311] alpha_s:  [0.1078311 0.0453311] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  8\n",
      "risks:  [-3.308 -1.405]  ; best risk:  [-3.308 -1.405]\n",
      "\n",
      "NMI scores:  0.0014229573542260932 acc:  0.37971830985915495\n",
      "Iteration:  10\n",
      "#### Iteration: 9 ; current alpha:  [0.05392 0.08514] [0.05392 0.08514] [0.05392 0.08514]\n",
      "Iteration: 9  k: 8 Improved minimax risk (arg/max):  [1] -2.338\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 9 ; step reduced minimax?: 0  risk (arg/ max) [1] [-2.338] ; (arg/ max best):  [1] -2.338\n",
      "alpha_l:  [0.05391555 0.08516555] alpha_s:  [0.05391555 0.08516555] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  8\n",
      "risks:  [-3.417 -2.338]  ; best risk:  [-3.417 -2.338]\n",
      "\n",
      "NMI scores:  0.0016404819323340126 acc:  0.37577464788732395\n",
      "Iteration:  11\n",
      "#### Iteration: 10 ; current alpha:  [0.02696 0.1051 ] [0.02696 0.1051 ] [0.02696 0.1051 ]\n",
      "Iteration:  10  k: 9 No minimax risk improvement, current best (arg/max):  [1] -2.338\n",
      "all_masks [0. 1.] 1.0 1.0 1.0\n",
      "Iteration: 10 ; step reduced minimax?: 1  risk (arg/ max) [1] [-1.445] ; (arg/ max best):  [1] -2.338\n",
      "alpha_l:  [0.02695778 0.10508278] alpha_s:  [0.02695778 0.10508278] ; new delta:  [0. 1.] [0. 1.]  mu:  0.5 ; K:  9\n",
      "risks:  [-4.105 -1.445]  ; best risk:  [-3.417 -2.338]\n",
      "\n",
      "NMI scores:  0.0031802332483697854 acc:  0.37915492957746477\n",
      "Iteration:  12\n",
      "dataset: imdb\t metric: jaccard similarity\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "alpha_l = [0.1, 0.1]\n",
    "alpha_s = [0.1, 0.1]\n",
    "alpha_u = [0.1, 0.1]\n",
    "lambda_s = 10\n",
    "lambda_u = 1e+3\n",
    "result = dict()\n",
    "result['imdb'] = debias_mining_model([G_mdm, G_mam], name='imdb', alpha_l=alpha_l, alpha_s=alpha_s, alpha_u=alpha_u, \n",
    "                                     lambda_s=1000, lambda_u = 1e+6, ncluster=3, metric='jaccard')\n",
    "with open('result/sc/model/jaccard_imdb.pickle', 'wb') as f:\n",
    "    pickle.dump(result, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "f_labels = result['imdb'][11]['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation for imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result 0.1\n",
    "\n",
    "vanilla acc: 0.3284507042253521\n",
    "\n",
    "fair acc: 0.37661971830985913"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla acc: 0.37633802816901407, nmi: 0.005000645791578157, f1: 0.21611338884827044\n",
      "fair acc: 0.37915492957746477, nmi: 0.0031802332483697854, f1: 0.19286743876870074, reduction:0.9999764280125271\n"
     ]
    }
   ],
   "source": [
    "def lp_diff(vanilla_result, fair_result, ord=None):\n",
    "    diffs = {}\n",
    "    if ord == 'fro':\n",
    "        diffs = 0\n",
    "        for i in range(vanilla_result.shape[1]):\n",
    "            residual = min(\n",
    "                np.linalg.norm(vanilla_result[:, i] + fair_result[:, i], ord=2),\n",
    "                np.linalg.norm(vanilla_result[:, i] - fair_result[:, i], ord=2)\n",
    "            )\n",
    "            diffs += (residual ** 2)\n",
    "        return np.sqrt(diffs)\n",
    "    else:\n",
    "        diff = vanilla_result - fair_result\n",
    "        return np.linalg.norm(diff, ord=ord)\n",
    "    \n",
    "def calc_bias(graphs, vanilla_result, fair_result, sim, metric='jaccard'):\n",
    " \n",
    "    vanilla_result = np.array(vanilla_result)\n",
    "    fair_result = np.array(fair_result)\n",
    "    # calculate bias\n",
    "    vanilla_bias = utils.trace(vanilla_result[1].T @ sim @ vanilla_result[1]) # vanilla bias\n",
    "    fair_bias = utils.trace(fair_result.T @ sim @ fair_result)  # fair bias\n",
    "    reduction = 1 - (fair_bias / vanilla_bias)\n",
    "    return reduction\n",
    "\n",
    "fair_result = result['imdb'][11]['U_mod']\n",
    "sim = result['imdb'][11]['L_s']\n",
    "bias = calc_bias([G_mdm, G_mam], vanilla_result, fair_result, sim)\n",
    "#diff = [lp_diff(vanilla_result[k], fair_result[k], ord='fro') / np.linalg.norm(vanilla_result[k], ord='fro') for k in range(2)]\n",
    "\n",
    "### accuracy\n",
    "def acc(true, pred):\n",
    "    acc = 0\n",
    "    for i in range(len(true)):\n",
    "        if true[i] == pred[i]:\n",
    "            acc += 1\n",
    "    return acc / len(true)\n",
    "\n",
    "vanilla_acc = acc(label, vanilla_labels[1])\n",
    "vanilla_f1 = f1_score(label, vanilla_labels[1], average='macro')\n",
    "vanilla_nmi = normalized_mutual_info_score(label, vanilla_labels[1])\n",
    "print('vanilla acc: {}, nmi: {}, f1: {}'.format(vanilla_acc, vanilla_nmi, vanilla_f1))\n",
    "\n",
    "\n",
    "## fairness\n",
    "fair_acc = acc(label, f_labels)\n",
    "fair_f1 = f1_score(label, f_labels, average='macro')\n",
    "fair_nmi = normalized_mutual_info_score(label, f_labels)\n",
    "print('fair acc: {}, nmi: {}, f1: {}, reduction:{}'.format(fair_acc, fair_nmi, fair_f1, bias))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = result['imdb'][11]['L']\n",
    "L_s = result['imdb'][11]['L_s']\n",
    "U = result['imdb'][11]['U']\n",
    "U_mod = result['imdb'][11]['U_mod']\n",
    "KU = result['imdb'][11]['KU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = utils.trace(U[0].T @ L[0] @ U[0]) + utils.trace(U[1].T @ L[1] @ U[1])\n",
    "loss_sim = utils.trace(U_mod.T @ L_s @ U_mod)\n",
    "loss_ku = utils.trace(U[0].T @ KU[0] @ U[0]) + utils.trace(U[1].T @ KU[1] @ U[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.7193893006709167 0.0007369281115751006 1.951075483938202e-14\n"
     ]
    }
   ],
   "source": [
    "print(loss, loss_sim, loss_ku)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "PATH = 'data/DBLP/dblp.pkl'\n",
    "dblp = pickle.load(open(PATH, 'rb'))\n",
    "\n",
    "G_pap = nx.convert_matrix.from_numpy_array(dblp['PAP'].astype(int))\n",
    "G_ppp = nx.convert_matrix.from_numpy_array(dblp['PPrefP'].astype(int))\n",
    "G_ptp = nx.convert_matrix.from_numpy_array(dblp['PTP'].astype(int))\n",
    "#G_patap = nx.convert_matrix.from_numpy_array(dblp['PATAP'].astype(int))\n",
    "\n",
    "n_clusters = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_result = vanilla([G_pap, G_ppp, G_ptp], 'dblp', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vanilla_result = vanilla([G_pap, G_ppp, G_ptp], 'dblp', 4)\n",
    "vanilla_kmeans = [KMeans(n_clusters=n_clusters, random_state=0, n_init=200).fit(vanilla_result[i]) \n",
    "                  for i in rane(len(vanilla_result))]\n",
    "vanilla_labels = [vanilla_kmeans[i].labels_ for i in rane(len(vanilla_result))]\n",
    "\n",
    "## vanilla\n",
    "label = []\n",
    "for li in dblp['label'].astype(int):\n",
    "    if li[0] == 1:\n",
    "        label.append(0)\n",
    "    elif li[1] == 1:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_l = [0.1, 0.1]\n",
    "alpha_s = [0.1, 0.1]\n",
    "alpha_u = [0.1, 0.1]\n",
    "lmbda = 0.5\n",
    "result = dict()\n",
    "result['imdb'] = debias_mining_model([G_mdm, G_mam], name='imdb', alpha_l=alpha_l,\n",
    "                                     alpha_s=alpha_s, alpha_u=alpha_u, lmbda=lmbda, ncluster=3, metric='jaccard')\n",
    "with open('result/sc/model/jaccard_imdb.pickle', 'wb') as f:\n",
    "    pickle.dump(result, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_result = result['imdb'][11]['U_mod']\n",
    "f_kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=200).fit(fair_result)\n",
    "f_labels = result['imdb'][11]['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation for DBLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla acc: 0.33887323943661973, nmi: 4.1485791205673264e-05 \n",
      "fair acc: 0.37661971830985913, nmi: 0.004848365378896624, reduction:0.9991040634126029\n"
     ]
    }
   ],
   "source": [
    "def lp_diff(vanilla_result, fair_result, ord=None):\n",
    "    diffs = {}\n",
    "    if ord == 'fro':\n",
    "        diffs = 0\n",
    "        for i in range(vanilla_result.shape[1]):\n",
    "            residual = min(\n",
    "                np.linalg.norm(vanilla_result[:, i] + fair_result[:, i], ord=2),\n",
    "                np.linalg.norm(vanilla_result[:, i] - fair_result[:, i], ord=2)\n",
    "            )\n",
    "            diffs += (residual ** 2)\n",
    "        return np.sqrt(diffs)\n",
    "    else:\n",
    "        diff = vanilla_result - fair_result\n",
    "        return np.linalg.norm(diff, ord=ord)\n",
    "    \n",
    "def calc_bias(graphs, vanilla_result, fair_result, sim, metric='jaccard'):\n",
    "    vanilla_result = np.array(vanilla_result)\n",
    "    fair_result = np.array(fair_result)\n",
    "    # calculate bias\n",
    "    vanilla_bias = utils.trace(vanilla_result[0].T @ sim @ vanilla_result[0]) # vanilla bias\n",
    "    fair_bias = utils.trace(fair_result.T @ sim @ fair_result)  # fair bias\n",
    "    reduction = 1 - (fair_bias / vanilla_bias)\n",
    "    return reduction\n",
    "\n",
    "\n",
    "sim = result['imdb'][11]['L_s']\n",
    "bias = calc_bias([G_mdm, G_mam], vanilla_result, fair_result, sim)\n",
    "#diff = [lp_diff(vanilla_result[k], fair_result[k], ord='fro') / np.linalg.norm(vanilla_result[k], ord='fro') for k in range(2)]\n",
    "\n",
    "### accuracy\n",
    "def acc(true, pred):\n",
    "    acc = 0\n",
    "    for i in range(len(true)):\n",
    "        if true[i] == pred[0][i]:\n",
    "            acc += 1\n",
    "    return acc / len(true)\n",
    "\n",
    "def accf(true, pred):\n",
    "    acc = 0\n",
    "    for i in range(len(true)):\n",
    "        if true[i] == pred[i]:\n",
    "            acc += 1\n",
    "    return acc / len(true)\n",
    "\n",
    "vanilla_acc = acc(label, vanilla_labels)\n",
    "vanilla_f1 = f1_score(label, vanilla_labels[0], average='macro')\n",
    "vanilla_nmi = normalized_mutual_info_score(label, vanilla_labels[0])\n",
    "print('vanilla acc: {}, nmi: {} '.format(vanilla_acc, vanilla_nmi))\n",
    "\n",
    "\n",
    "## fairness\n",
    "fair_acc = accf(label, f_labels)\n",
    "fair_f1 = f1_score(label, f_labels, average='macro')\n",
    "fair_nmi = normalized_mutual_info_score(label, f_labels)\n",
    "print('fair acc: {}, nmi: {}, reduction:{}'.format(fair_acc, fair_nmi, bias))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter social/mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/Twitter/higgs-social_network.edgelist'\n",
    "#G_social = nx.read_edgelist('data/Twitter/higgs-{}_network.edgelist'.format('social'), \n",
    "#                            create_using=nx.Graph(), nodetype=int, data=(('weight', float),))\n",
    "G_retweet = nx.read_edgelist('data/Twitter/higgs-{}_network.edgelist'.format('retweet'), \n",
    "                             create_using=nx.Graph(), nodetype=int, data=(('weight', float),))\n",
    "G_reply = nx.read_edgelist('data/Twitter/higgs-{}_network.edgelist'.format('reply'), \n",
    "                             create_using=nx.Graph(), nodetype=int, data=(('weight', float),))\n",
    "#G_mention = nx.read_edgelist('data/Twitter/higgs-{}_network.edgelist'.format('mention'), \n",
    "#                             create_using=nx.Graph(), nodetype=int, data=(('weight', float),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "udict = {}\n",
    "def vanilla(graph, name):\n",
    "    print(\"start lcc...\")\n",
    "    lcc = max(nx.connected_components(graph), key=len)  # take largest connected components\n",
    "    print(\"start adj...\")\n",
    "    adj = nx.to_scipy_sparse_matrix(graph, nodelist=lcc, dtype='float', format='csc')\n",
    "    ## unnormalized laplacian\n",
    "    lap = laplacian(adj)\n",
    "    \n",
    "    lap_minus = (-1) * lap\n",
    "    #lap *= -1\n",
    "    print(\"start eigen...\")\n",
    "    _, u = eigsh(lap_minus, which='LM', k=10, sigma=1.0)\n",
    "    udict[name] = dict()\n",
    "    udict[name]['eigenvectors'] = u\n",
    "\n",
    "    with open('result/sc/vanilla_'+name+'.pickle', 'wb') as f:\n",
    "        pickle.dump(udict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 10\n",
    "social_result = vanilla(G_reply, 'reply')\n",
    "retweet_result = vanilla(G_retweet, 'retweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_kmeans_reply = KMeans(n_clusters=n_clusters, random_state=0, n_init=1).fit(social_result)\n",
    "vanilla_labels_reply = vanilla_kmeans_reply.labels_\n",
    "\n",
    "vanilla_kmeans_retweet = KMeans(n_clusters=n_clusters, random_state=0, n_init=1).fit(retweet_result)\n",
    "vanilla_labels_retweet = vanilla_kmeans_retweet.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's debias the mining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla(graph, name, n_clusters):\n",
    "    udict = {}\n",
    "    if name == 'graph':\n",
    "        lcc = max(nx.connected_components(graph), key=len)  # take largest connected components\n",
    "        adj = nx.to_scipy_sparse_matrix(graph, nodelist=lcc, dtype='float', format='csc')\n",
    "    elif name == 'synthetic':\n",
    "        adj_1 = graph['adjs'][0]\n",
    "        adj_2 = graph['adjs'][1]\n",
    "    elif name == 'imdb':\n",
    "        #lcc_1 = max(nx.connected_components(graph[0]), key=len)  # take largest connected components\n",
    "        #lcc_2 = max(nx.connected_components(graph[1]), key=len)\n",
    "        adj_1 = nx.to_scipy_sparse_matrix(graph[0], nodelist=set(graph[0].nodes), dtype='float', format='csc')\n",
    "        adj_2 = nx.to_scipy_sparse_matrix(graph[1], nodelist=set(graph[1].nodes), dtype='float', format='csc')\n",
    "    elif name == 'twitter':\n",
    "        #lcc_1 = max(nx.connected_components(graph[0]), key=len)  # take largest connected components\n",
    "        #lcc_2 = max(nx.connected_components(graph[1]), key=len)\n",
    "        adj_1 = nx.to_scipy_sparse_matrix(graph[0], nodelist=set(graph[0].nodes), dtype='float', format='csc')\n",
    "        adj_2 = nx.to_scipy_sparse_matrix(graph[1], nodelist=set(graph[1].nodes), dtype='float', format='csc')\n",
    "    ## unnormalized laplacian\n",
    "    lap_1 = laplacian(adj_1)\n",
    "    lap_2 = laplacian(adj_2)\n",
    "    \n",
    "    lap1_minus = (-1) * lap_1\n",
    "    lap2_minus = (-1) * lap_2\n",
    "    #lap *= -1\n",
    "    _, u_1 = eigsh(lap1_minus, which='LM', k=n_clusters, sigma=1.0)\n",
    "    _, u_2 = eigsh(lap2_minus, which='LM', k=n_clusters, sigma=1.0)\n",
    "    udict[name] = dict()\n",
    "    u = [u_1, u_2]\n",
    "    udict[name]['eigenvectors'] = u\n",
    "\n",
    "    with open('result/sc/vanilla_'+name+'.pickle', 'wb') as f:\n",
    "        pickle.dump(udict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return u\n",
    "\n",
    "## 2n+1 parameters\n",
    "def spectral_clustering(adjs, sims, alpha_l, alpha_s, alpha_u, lambda_s, lambda_u, ncluster, niteration, v0,\n",
    "                        max_patience, K_ini, K_min, mu,\n",
    "                        risk_round_factor, reset_optimizer):\n",
    "        ## initialize\n",
    "        i = 0\n",
    "        i_patience = 0\n",
    "\n",
    "        alpha_l = np.array(alpha_l)\n",
    "        alpha_s = np.array(alpha_s)\n",
    "        alpha_u = np.array(alpha_u)\n",
    "\n",
    "\n",
    "        L = {}\n",
    "        U = {}\n",
    "        \n",
    "        ## save \n",
    "        matrices = {0: {}}\n",
    "        matrices[0]['L'] = {}\n",
    "        matrices[0]['U'] = {}\n",
    "        matrices[0]['S'] = {}\n",
    "        matrices[0]['risk'] = []\n",
    "        matrices[0]['alpha_l'] = []\n",
    "        matrices[0]['alpha_s'] = []\n",
    "        matrices[0]['alpha_u'] = []\n",
    "        matrices[0]['risk_best'] = []\n",
    "        matrices[0]['alpha_s_best'] = []\n",
    "        matrices[0]['alpha_u_best'] = []\n",
    "        matrices[0]['params'] = []\n",
    "\n",
    "        for v in range(len(adjs)):\n",
    "            # initial U\n",
    "            matrices[0]['S'][v] = alpha_s[v] * lambda_s * laplacian(sims[v]) \n",
    "            L[v] = alpha_l[v] * laplacian(adjs[v]) + alpha_s[v] * lambda_s * laplacian(sims[v])\n",
    "            L_minus = (-1) * L[v]\n",
    "\n",
    "            _, U[v] = eigsh(L_minus, which='LM', k=ncluster, sigma=1.0, v0=v0)\n",
    "        \n",
    "        K = K_ini\n",
    "        while( i <= niteration) & (i_patience <= max_patience):\n",
    "            matrices[i + 1] = {}\n",
    "            matrices[i + 1]['L'] = {}\n",
    "            matrices[i + 1]['U'] = {}\n",
    "            matrices[i + 1]['S'] = {}\n",
    "            matrices[i + 1]['KU'] = {}\n",
    "            matrices[i + 1]['params'] = []\n",
    "            matrices[i + 1]['risk'] = []\n",
    "            matrices[i + 1]['alpha_l'] = []\n",
    "            matrices[i + 1]['alpha_s'] = []\n",
    "            matrices[i + 1]['alpha_u'] = []\n",
    "            matrices[i + 1]['risk_best'] = []\n",
    "            matrices[i + 1]['alpha_l_best'] = []\n",
    "            matrices[i + 1]['alpha_s_best'] = []\n",
    "            matrices[i + 1]['alpha_u_best'] = []\n",
    "            matrices[i + 1]['nmi_scores'] = {}\n",
    "            matrices[i + 1]['labels'] = {}\n",
    "            \n",
    "            alpha_l_penalty = (alpha_l).astype('float16')\n",
    "            alpha_s_penalty = (alpha_s).astype('float16')\n",
    "            alpha_u_penalty = (alpha_u).astype('float16')\n",
    "            \n",
    "            print('#### Iteration:', i, '; current alpha: ', alpha_l_penalty, alpha_s_penalty, alpha_u_penalty)\n",
    "            loss = []\n",
    "            \n",
    "            \n",
    "            for v in range(len(adjs)):\n",
    "                u1 = sparse.csr_matrix(U[v])\n",
    "                loss.append((u1.T @ L[v] @ u1).diagonal().sum())\n",
    "            base_loss = np.array(loss)\n",
    "\n",
    "            # h,risk,_ = bs_optimal(mu_i)\n",
    "            risk = np.round(base_loss + 0, risk_round_factor)\n",
    "            risk_max = np.max(risk)\n",
    "\n",
    "            # argmax_risks\n",
    "            argrisk_max = np.arange(risk.shape[0])\n",
    "            argrisk_max = argrisk_max[risk == risk_max] #consider argmax exactly\n",
    "        \n",
    "            ## initialize\n",
    "            if i == 0:\n",
    "                risk_max_best = risk_max + 1\n",
    "                \n",
    "            if risk_max_best > risk_max: # risk is improved\n",
    "                # update best risk\n",
    "                risk_max_best = risk_max \n",
    "                argrisk_max_best = argrisk_max \n",
    "                risk_best = risk \n",
    "                alpha_l_best = alpha_l\n",
    "                alpha_s_best = alpha_s\n",
    "                alpha_u_best = alpha_u\n",
    "\n",
    "                ## resets\n",
    "                K = np.minimum(K, K_min)\n",
    "                i_patience = 0\n",
    "                type_step = 0 #improvement\n",
    "\n",
    "                print('Iteration:', i,' k:',K,'Improved minimax risk (arg/max): ', argrisk_max_best, risk_max_best)\n",
    "                #f.write('Iteration:', i,' k:',K,'Improved minimax risk (arg/max): ', argrisk_max_best, risk_max_best)\n",
    "            else:\n",
    "                K += 1\n",
    "                i_patience += 1\n",
    "                type_step = 1 # no improvement\n",
    "                print('Iteration: ', i,' k:',K, 'No minimax risk improvement, current best (arg/max): ', \n",
    "                      argrisk_max_best, risk_max_best)\n",
    "                #f.write('Iteration: ', i,' k:',K, 'No minimax risk improvement, current best (arg/max): ', \n",
    "                #      argrisk_max_best, risk_max_best)\n",
    "            \n",
    "            ## step update\n",
    "            mask_alpha_l = np.zeros(alpha_l.shape)\n",
    "            mask_alpha_s = np.zeros(alpha_s.shape)\n",
    "            mask_alpha_u = np.zeros(alpha_u.shape)\n",
    "            \n",
    "            mask_alpha_l[risk >= risk_max_best] = 1\n",
    "            mask_alpha_s[risk >= risk_max_best] = 1\n",
    "            mask_alpha_u[risk >= risk_max_best] = 1\n",
    "        \n",
    "            step_alpha_l = mask_alpha_l / np.sum(mask_alpha_l)\n",
    "            step_alpha_s = mask_alpha_s / np.sum(mask_alpha_s)\n",
    "            step_alpha_u = mask_alpha_u / np.sum(mask_alpha_u)\n",
    "            print('all_masks', mask_alpha_l, np.sum(mask_alpha_l), np.sum(mask_alpha_s), np.sum(mask_alpha_u))\n",
    "                \n",
    "            ##############    Save lists  ##############\n",
    "            #weight vector updated after save lists\n",
    "            matrices[i + 1]['params'].append([type_step, K, mu])\n",
    "            matrices[i + 1]['risk'].append(risk)\n",
    "            matrices[i + 1]['alpha_l'].append(alpha_l)\n",
    "            matrices[i + 1]['alpha_s'].append(alpha_s)\n",
    "            matrices[i + 1]['alpha_u'].append(alpha_u)\n",
    "            matrices[i + 1]['risk_best'].append(risk_best)\n",
    "            matrices[i + 1]['alpha_l_best'].append(alpha_l_best)\n",
    "            matrices[i + 1]['alpha_s_best'].append(alpha_s_best)\n",
    "            matrices[i + 1]['alpha_u_best'].append(alpha_u_best)\n",
    "            \n",
    "            print('Iteration:', i, '; step reduced minimax?:', type_step, ' risk (arg/ max)', argrisk_max,\n",
    "                  risk[argrisk_max], '; (arg/ max best): ', argrisk_max_best, risk_max_best)\n",
    "            print('alpha_l: ', alpha_l, 'alpha_s: ', alpha_s, '; new delta: ', step_alpha_l, step_alpha_s, ' mu: ', mu, '; K: ', K)\n",
    "            print('risks: ', risk, ' ; best risk: ', risk_best)\n",
    "            print()\n",
    "\n",
    "            \n",
    "            ############################################\n",
    "            for view, u in U.items():\n",
    "                view_kmeans = KMeans(n_clusters=u.shape[1], random_state=42, n_init=5).fit(u)\n",
    "                view_labels = view_kmeans.labels_\n",
    "                matrices[i + 1]['labels'][view] = view_labels\n",
    "                #matrices[i + 1]['nmi_scores'][view] = normalized_mutual_info_score(view_labels, vanilla_labels[view])\n",
    "                #matrices[i + 1]['nmi_scores'][view] = normalized_mutual_info_score(label, view_labels)\n",
    "\n",
    "            #print(\"NMI scores: \", matrices[i + 1]['nmi_scores'])\n",
    "            ### UPDATE WEIGHTING_VECTOR ###\n",
    "            alpha_l = (1 - mu) * alpha_l + step_alpha_l * mu / K\n",
    "            alpha_s = (1 - mu) * alpha_s + step_alpha_s * mu / K\n",
    "            alpha_u = (1 - mu) * alpha_u + step_alpha_u * mu / K\n",
    "            \n",
    "\n",
    "            i += 1\n",
    "                \n",
    "\n",
    "            for j in range(len(adjs)):\n",
    "                KU = sum(np.matmul(U[k], U[k].T) for k in range(len(U)) if k != j)\n",
    "                lap = alpha_l[j] * L[j] - alpha_u[j] * lambda_u * KU + alpha_s[j] * lambda_s * laplacian(sims[j])\n",
    "                \n",
    "                lap_minus = (-1) * lap\n",
    "                _, U[j] = eigsh(lap_minus, which='LM', k=ncluster, sigma=1.0, v0=v0)\n",
    "                L[j] = np.array(lap)\n",
    "\n",
    "                matrices[i]['L'][j] = L[j]\n",
    "                matrices[i]['U'][j] = U[j]\n",
    "                matrices[i]['KU'][j] = KU\n",
    "                matrices[i]['S'][j] = alpha_s[j] * lambda_s * laplacian(sims[j]) \n",
    "\n",
    "            print('Iteration: ', i+1)\n",
    "            #f.write('Iteration: ', i+1)\n",
    "\n",
    "\n",
    "\n",
    "        #return matrices\n",
    "        return matrices[i]\n",
    "    \n",
    "def debias_mining_model(graphs, name, alpha_l, alpha_s, alpha_u, lmbda, ncluster, niter=20, metric=None):\n",
    "    # build adjacency matrix\n",
    "    adj_1 = nx.to_scipy_sparse_matrix(graphs[0], nodelist=set(graphs[0].nodes), dtype='float', format='csc')\n",
    "    adj_2 = nx.to_scipy_sparse_matrix(graphs[1], nodelist=set(graphs[1].nodes), dtype='float', format='csc')\n",
    "\n",
    "    # build similarity matrix\n",
    "    sim_1 = utils.get_similarity_matrix(adj_1, metric=metric)\n",
    "    sim_2 = utils.get_similarity_matrix(adj_2, metric=metric)\n",
    "    \n",
    "    adjs = np.array([adj_1, adj_2])\n",
    "    sims = np.array([sim_1, sim_2])\n",
    "    # debias spectral clustering\n",
    "    \n",
    "    #alpha_l = alpha_\n",
    "    #alpha_s = [alpha, alpha]\n",
    "    #alpha_u = [alpha, alpha]\n",
    "    lambda_s = lmbda\n",
    "    lambda_u = lmbda\n",
    "    \n",
    "    # V, U = sc.debias_alg(adj, sim, alpha, ncluster=10, v0=v0[name])\n",
    "    # model = DebiasModelFair(adjs, sims, ncluster=10, niteration=niter, v0=v0[name])\n",
    "    result = spectral_clustering(adjs, sims, alpha_l, alpha_s, alpha_u, lambda_s, lambda_u,\n",
    "                                 ncluster=n_clusters, niteration=niter, v0=None,\n",
    "                                 max_patience=15, K_ini=1, K_min=20, mu=0.5,\n",
    "                                 risk_round_factor=3, reset_optimizer=False)\n",
    "    \n",
    "\n",
    "    print('dataset: {}\\t metric: {} similarity'.format(name, metric))\n",
    "    print('Finished!')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 10\n",
    "social_result = vanilla(G_reply, 'reply')\n",
    "retweet_result = vanilla(G_retweet, 'retweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_kmeans_reply = KMeans(n_clusters=n_clusters, random_state=0, n_init=1).fit(social_result)\n",
    "vanilla_labels_reply = vanilla_kmeans_reply.labels_\n",
    "\n",
    "vanilla_kmeans_retweet = KMeans(n_clusters=n_clusters, random_state=0, n_init=1).fit(retweet_result)\n",
    "vanilla_labels_retweet = vanilla_kmeans_retweet.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-238cac96e565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlmbda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m result['imdb'] = debias_mining_model([G_reply, G_retweet], name='twitter', alpha_l=alpha_l,\n\u001b[0m\u001b[1;32m      7\u001b[0m                                      alpha_s=alpha_s, alpha_u=alpha_u, lmbda=lmbda, ncluster=3, metric='jaccard')\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'result/sc/model/jaccard_imdb.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-08cec2ae7dd9>\u001b[0m in \u001b[0;36mdebias_mining_model\u001b[0;34m(graphs, name, alpha_l, alpha_s, alpha_u, lmbda, ncluster, niter, metric)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;31m# V, U = sc.debias_alg(adj, sim, alpha, ncluster=10, v0=v0[name])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;31m# model = DebiasModelFair(adjs, sims, ncluster=10, niteration=niter, v0=v0[name])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     result = spectral_clustering(adjs, sims, alpha_l, alpha_s, alpha_u, lambda_s, lambda_u,\n\u001b[0m\u001b[1;32m    239\u001b[0m                                  \u001b[0mncluster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mniteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mniter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                                  \u001b[0mmax_patience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK_ini\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-08cec2ae7dd9>\u001b[0m in \u001b[0;36mspectral_clustering\u001b[0;34m(adjs, sims, alpha_l, alpha_s, alpha_u, lambda_s, lambda_u, ncluster, niteration, v0, max_patience, K_ini, K_min, mu, risk_round_factor, reset_optimizer)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mL_minus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meigsh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL_minus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mncluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK_ini\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/scipy/sparse/linalg/eigen/arpack/arpack.py\u001b[0m in \u001b[0;36meigsh\u001b[0;34m(A, k, M, sigma, which, v0, ncv, maxiter, tol, return_eigenvectors, Minv, OPinv, mode)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_ARPACK_LOCK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverged\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m             \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_eigenvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/scipy/sparse/linalg/eigen/arpack/arpack.py\u001b[0m in \u001b[0;36miterate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0mBxslice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipntr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipntr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myslice\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBxslice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mido\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myslice\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxslice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/scipy/sparse/linalg/interface.py\u001b[0m in \u001b[0;36mmatvec\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/scipy/sparse/linalg/eigen/arpack/arpack.py\u001b[0m in \u001b[0;36m_matvec\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    924\u001b[0m                     + 1j * self.M_lu.solve(np.imag(x).astype(self.dtype)))\n\u001b[1;32m    925\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM_lu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alpha_l = [1, 0.03]\n",
    "alpha_s = [1, 0.03]\n",
    "alpha_u = [1, 0.03]\n",
    "lmbda = 5\n",
    "result = dict()\n",
    "result['imdb'] = debias_mining_model([G_reply, G_retweet], name='twitter', alpha_l=alpha_l,\n",
    "                                     alpha_s=alpha_s, alpha_u=alpha_u, lmbda=lmbda, ncluster=3, metric='jaccard')\n",
    "with open('result/sc/model/jaccard_imdb.pickle', 'wb') as f:\n",
    "    pickle.dump(result, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\"\"\"f_kmeans = [KMeans(n_clusters=n_clusters, random_state=0, n_init=200).fit(result['imdb']['U'][0]),\n",
    "            KMeans(n_clusters=n_clusters, random_state=0, n_init=200).fit(result['imdb']['U'][1])]\n",
    "f_labels = [f_kmeans[0].labels_, f_kmeans[1].labels_]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
